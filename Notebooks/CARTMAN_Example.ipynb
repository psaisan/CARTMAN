{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import UpSet, from_memberships, plot\n",
    "import itertools\n",
    "\n",
    "# Add Path to HOMER functions:\n",
    "# HOMER has to be installed in your system first\n",
    "os.environ[\"PATH\"] += \"/bioinformatics/anaconda3_052020/condabin:/bioinformatics/A.C.Rsuite:/bioinformatics/bowtie2:/bioinformatics/BSseeker2:/bioinformatics/adapterremoval:/bioinformatics/idr/bin:/bioinformatics/glassutils/scripts:/bioinformatics/STAR/bin/Linux_x86_64_static:/bioinformatics/scripts:/bioinformatics/sratoolkit/bin:/bioinformatics/FastQC:/bioinformatics/bedtools/bin:/bioinformatics/samtools/bin:/bioinformatics/anaconda3_052020/bin:/bioinformatics/homer/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/local/lib:/bioinformatics/anaconda3_052020/condabin:/bioinformatics/A.C.Rsuite:/bioinformatics/bowtie2:/bioinformatics/BSseeker2:/bioinformatics/adapterremoval:/bioinformatics/idr/bin:/bioinformatics/glassutils/scripts:/bioinformatics/STAR/bin/Linux_x86_64_static:/bioinformatics/scripts:/bioinformatics/sratoolkit/bin:/bioinformatics/FastQC:/bioinformatics/bedtools/bin:/bioinformatics/samtools/bin:/bioinformatics/anaconda3_052020/bin:/bioinformatics/homer/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/local/lib:\"\n",
    "#os.environ[\"PATH\"] += \"/bioinformatics/homer/bin/\"\n",
    "\n",
    "#import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def read_motif_headers(motif_file):\n",
    "    \n",
    "    \"\"\"Read headers from a motif file to extract motif names from header lines.\"\"\"\n",
    "    # Regex pattern to capture the motif name right after '>' and before the first tab\n",
    "\n",
    "    if is_denovo_motif_file(motif_file):\n",
    "        print('DENOVO HOMER')\n",
    "        return read_motif_headers_homer(motif_file)\n",
    "    else:\n",
    "        print('Not DEVONVO HOMER')\n",
    "        return read_motif_headers_jaspar(motif_file)   \n",
    "\n",
    "\n",
    "def is_denovo_motif_file(filename):\n",
    "    \"\"\"\n",
    "    Reads a file and checks if any header lines starting with '>' contain \"BestGuess:\".\n",
    "\n",
    "    :param filename: str, the path to the file to be read\n",
    "    :return: bool, True if any header line contains \"BestGuess:\", otherwise False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('>') and \"BestGuess:\" in line:\n",
    "                    return True\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filename}' does not exist.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_motif_headers_jaspar(motif_file):\n",
    "    \n",
    "    \"\"\"Read headers from a motif file to extract motif names from header lines.\"\"\"\n",
    "    # Regex pattern to capture the motif name right after '>' and before the first tab\n",
    "    pattern = r'^>(\\w+)\\t'\n",
    "    motif_names = []\n",
    "    \n",
    "    with open(motif_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):  # Check if the line is a header\n",
    "                match = re.match(pattern, line)\n",
    "                if match:\n",
    "                    motif_names.append(match.group(1))  # Add the motif name to the list\n",
    "    \n",
    "    print(motif_names)\n",
    "    return motif_names\n",
    "\n",
    "\n",
    "\n",
    "def read_motif_headers_homer(motif_file):\n",
    "    \"\"\"Read headers from a motif file to extract motif names.\"\"\"\n",
    "    pattern = r'^>(\\w+?)\\t'\n",
    "    with open(motif_file, 'r') as file:\n",
    "        headers = [line.strip() for line in file if line.startswith('>')]\n",
    " \n",
    "    pattern = r\"BestGuess:([^/]+)\"\n",
    "    names = []\n",
    "\n",
    "    for item in headers:\n",
    "        names.extend( re.findall(pattern, item) )\n",
    " \n",
    "    return names \n",
    "\n",
    "\n",
    "def get_motif_count(peak_file, genome, motif_file, force=False):\n",
    "    \"\"\"Get motif count using HOMER annotatePeaks.\"\"\"\n",
    "    \n",
    "    \n",
    "    base_file = os.path.splitext(os.path.basename(peak_file))[0]\n",
    "    motif_base_file = os.path.splitext(os.path.basename(motif_file))[0]\n",
    "    \n",
    "    \n",
    "    base_path = os.path.dirname(os.path.abspath(peak_file))\n",
    "    \n",
    "    motif_count_file = os.path.join(base_path, \"motifCount_counts_\"+base_file+'_'+motif_base_file+'.txt')\n",
    "    print('Motif_Count_File=',motif_count_file,'Force=',force)\n",
    "    command = f'annotatePeaks.pl {peak_file} {genome} -cpu 12 -noann -nogene -m {motif_file} -nmotifs > {motif_count_file}'\n",
    "    \n",
    "    if (not(os.path.exists(motif_count_file))):\n",
    "        print('FILE DOES NOT EXIST: ',motif_count_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (not(os.path.exists(motif_count_file)) or force==True):\n",
    "        print(f'{command}')\n",
    "        print('Generating '+motif_count_file)\n",
    "        os.system(command)\n",
    "    else :\n",
    "        print(f'** Loading ** ' + motif_count_file)\n",
    "\n",
    "        \n",
    "    return pd.read_table(motif_count_file, sep=\"\\t\"),  motif_count_file\n",
    "\n",
    "\n",
    "\n",
    "def transform_combinations_to_matrix(series, peak_file):\n",
    "   \n",
    "    motifs = set()\n",
    "    for combo in series.index:\n",
    "        motifs.update(motif.strip() for motif in combo.split(','))\n",
    "    \n",
    "\n",
    "    motif_list = sorted(motifs)  # Sorting to maintain a consistent order\n",
    "\n",
    "    \n",
    "    motif_list = list(set(motif_list))\n",
    "\n",
    "    peak_data = pd.read_csv(peak_file, sep='\\t')\n",
    "\n",
    "    # Get the total number of peaks\n",
    "    total_sum = len(peak_data)\n",
    "    \n",
    "    # Initialize a DataFrame to store the binary representation of combinations\n",
    "    data = {motif: [] for motif in motif_list}\n",
    "    data['Count'] = []  # Additional column for counts\n",
    "    data['Norm_Count'] = []  # Additional column for counts normalized\n",
    "    data['Motif Subset'] = []  # Additional column to store the set of motifs as a string\n",
    "    data[f'code-{peak_file}'] = []  # Additional column for binary code\n",
    "    \n",
    "    # Populate the DataFrame\n",
    "    for combo, count in series.items():\n",
    "        \n",
    "        current_row = {motif: 0 for motif in motif_list}\n",
    "        current_row['Count'] = count\n",
    "        current_row['Norm_Count'] = count / total_sum\n",
    "        current_row['Motif Subset'] = ','.join(sorted(combo.split(',')))  # Store the motif names as a sorted string\n",
    "        binary_code = 0\n",
    "\n",
    "      \n",
    "        for motif in motif_list:\n",
    "            #print('motif_list=',motif_list)\n",
    "            #print('combo=',combo)\n",
    "            #print( (motif.strip() for motif in combo.split(',')))\n",
    "            \n",
    "            binary_code <<= 1  # Shift left for each motif position\n",
    "            if motif in [motif.strip() for motif in combo.split(',')]: #      combo.split(','):\n",
    "                #print(motif,' is in current group - setting flag=1')\n",
    "                current_row[motif] = 1\n",
    "                binary_code |= 1  # Set the last bit to 1 if the motif is present\n",
    "\n",
    "        current_row[f'code-{peak_file}'] = binary_code\n",
    "        for key, value in current_row.items():\n",
    "            data[key].append(value)\n",
    "\n",
    "    # Create DataFrame from data dictionary\n",
    "    result_df = pd.DataFrame(data)\n",
    "    #display(result_df)  # Display the DataFrame for verification\n",
    "    return result_df\n",
    "\n",
    "def create_motif_network(data0, min_node_size=1000, max_node_size=5000):\n",
    "    \"\"\"\n",
    "    Create a network graph based on motif co-occurrences.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with columns representing motifs (M1, M2, ...) and a 'Count' column indicating\n",
    "                             the frequency of each motif combination.\n",
    "        min_node_size (int, optional): Minimum size of the node bubbles. Default is 100.\n",
    "        max_node_size (int, optional): Maximum size of the node bubbles. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data= data0.copy()\n",
    "    # Remove rows where 'Count' is 0\n",
    "    data = data[data['Count'] >  0] #min_subset_count]\n",
    "    \n",
    "   \n",
    "    data = data[data.iloc[:, :-1].sum(axis=1) > min_motif_set_count]\n",
    "    \n",
    "  \n",
    "    # Normalize the 'Count' column\n",
    "\n",
    "    max_count = data['Count'].max()\n",
    "    data['normalized_count'] = (data['Count'] / max_count) \n",
    "    \n",
    "    #print(data.columns)\n",
    "\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges based on co-occurrence\n",
    "    for _, row in data.iterrows():\n",
    "        #print(row)\n",
    "        motifs = [col for col in data.columns if row[col] == 1 and col not in ['Count', 'normalized_count']]\n",
    "        #print(motifs)\n",
    "        for i, motif1 in enumerate(motifs):\n",
    "           # print('m1',motif1)\n",
    "            for motif2 in motifs[i+1:]:\n",
    "                #print('m2',motif2)\n",
    "                if G.has_edge(motif1, motif2):\n",
    "                    G[motif1][motif2]['weight'] +=  ( row['Count'] +0  )\n",
    "                else:\n",
    "                    G.add_edge(motif1, motif2, weight= ( 0+row['Count'])) \n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    \n",
    "    # Calculate node sizes based on their total normalized co-occurrence count\n",
    "    node_weights = defaultdict(int)\n",
    "    for _, row in data.iterrows():\n",
    "        for motif in [col for col in data.columns if row[col] == 1 and col not in ['Count', 'normalized_count']]:\n",
    "            node_weights[motif] +=  (1*0+ row['normalized_count'])\n",
    "\n",
    "    # Scale node sizes between min_node_size and max_node_size\n",
    "    min_weight = min(node_weights.values())\n",
    "    max_weight = max(node_weights.values())\n",
    "    \n",
    "    node_size = [\n",
    "        min_node_size + (node_weights[motif] - min_weight) / (max_weight - min_weight) * (max_node_size - min_node_size)\n",
    "        for motif in G.nodes()\n",
    "    ]\n",
    "\n",
    " \n",
    "    fixed_edge_width=1\n",
    "    \n",
    "            # Calculate edge widths\n",
    "    if fixed_edge_width is not None:\n",
    "        edge_width = [fixed_edge_width for _ in G.edges()]\n",
    "    else:\n",
    "        edge_width = [G[u][v]['weight'] for u, v in G.edges()]                \n",
    "                    \n",
    "\n",
    "    # Create labels with motif names and their total occurrences\n",
    "    labels = {motif: f\"{motif}: \\n {int(node_weights[motif] * max_count)}\" for motif in G.nodes()}\n",
    "\n",
    "    # Draw the network\n",
    "    #pos = nx.spring_layout(G)\n",
    "    # Draw the network\n",
    "    pos = nx.spring_layout(G, k=10/np.sqrt(G.order()),scale=2, iterations=2000,seed=1234)  # Adjust k and iterations for more even spread\n",
    "\n",
    "        # Enforce maximum edge length\n",
    "    def enforce_max_edge_length(pos, max_length):\n",
    "        for u, v in G.edges():\n",
    "            dx = pos[v][0] - pos[u][0]\n",
    "            dy = pos[v][1] - pos[u][1]\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "            \n",
    "            if distance > max_length:\n",
    "                factor = max_length / (distance)\n",
    "                mid_x = (pos[u][0] + pos[v][0]) / 2\n",
    "                mid_y = (pos[u][1] + pos[v][1]) / 2\n",
    "                pos[u] = (mid_x - factor * (mid_x - pos[u][0]), mid_y - factor * (mid_y - pos[u][1]))\n",
    "                pos[v] = (mid_x + factor * (mid_x - pos[v][0]), mid_y + factor * (mid_y - pos[v][1]))\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue',edgecolors='red', linewidths=1.5)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "\n",
    "    plt.title('Motif Co-occurrence Network')\n",
    "    \n",
    "    \n",
    "def all_possible_combinations33(df, min_set_count=0):\n",
    "\n",
    "    # Get the list of motifs from the DataFrame columns\n",
    "    motifs = df.columns.tolist()\n",
    "    \n",
    "    # Create a dictionary to store the count of each combination, including \"None Present\"\n",
    "    combination_counts = {}\n",
    "    \n",
    "    # Check for rows where none of the motifs are present\n",
    "    none_present_count = df.eq(0).all(axis=1).sum()\n",
    "    combination_counts['0_Set'] = none_present_count  # Store count under the key 'None'   \n",
    "\n",
    "    # Generate combinations \n",
    "    for r in range(1, len(motifs) + 1):\n",
    "        for comb in itertools.combinations(motifs, r):\n",
    "            # Convert combination to a list to use as DataFrame selector\n",
    "            comb_list = list(comb)\n",
    "            \n",
    "            \n",
    "            if (len(comb_list) >= min_set_count):\n",
    "                #print(len(comb_list), 'COMB LIST :' , comb_list)\n",
    "                \n",
    "                # Create a mask that selects only rows where all motifs in the combo are 1\n",
    "                selected_motifs = df[comb_list].all(axis=1)  # True where all selected motifs are present\n",
    "                non_selected_motifs = df.drop(comb_list, axis=1, errors='ignore').eq(0).all(axis=1)  # True where all non-selected motifs are absent\n",
    "\n",
    "                # Combine masks to find rows that exactly match the combination\n",
    "                exact_match_mask = selected_motifs & non_selected_motifs\n",
    "\n",
    "                # Calculate the sum of rows that exactly match the combination\n",
    "                count = exact_match_mask.sum()\n",
    "\n",
    "                # Convert tuple to a comma-separated string for easier reading and use in plots\n",
    "                combination_key = ','.join(comb)\n",
    "                combination_counts[combination_key] = count\n",
    "    \n",
    "    return pd.Series(combination_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_motif_co_occurrences(df, max_distance):\n",
    "    category_counts = defaultdict(int)\n",
    "    \n",
    "    # Collect the original order of motifs\n",
    "    original_motifs = df.columns[1:].tolist()  # Exclude the first column which is PeakID\n",
    "    motif_columns = {motif: idx for idx, motif in enumerate(original_motifs)}\n",
    "    \n",
    "    # Second pass to build the binary matrix\n",
    "    binary_matrix = []\n",
    "    peak_names = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        co_occurrences = find_co_occurring_motifs(row[1:], max_distance)  # Exclude PeakID column\n",
    "        \n",
    "        if co_occurrences:\n",
    "            base_peak_id = row[0].rsplit('-', 1)[0]  # Remove the last part after '-'\n",
    "            start_index = int(row[0].rsplit('-', 1)[1])  # Get the starting index (the number after '-')\n",
    "            \n",
    "            for k, occurrence in enumerate(co_occurrences):\n",
    "                binary_row = [0] * len(original_motifs)\n",
    "                for motif in occurrence:\n",
    "                    if motif in motif_columns:  # Ensure only motifs in original columns are considered\n",
    "                        binary_row[motif_columns[motif]] = 1\n",
    "                category_counts[frozenset(occurrence)] += 1\n",
    "                \n",
    "                peak_id = f\"{base_peak_id}-{start_index + k}\"  # Increment the index for each co-occurrence\n",
    "                binary_matrix.append(binary_row)\n",
    "                peak_names.append(peak_id)\n",
    "    \n",
    "    # Create DataFrame from the binary matrix\n",
    "    binary_df = pd.DataFrame(binary_matrix, columns=original_motifs)\n",
    "    \n",
    "    # Add peak names as an additional column if needed for reference\n",
    "    binary_df.insert(0, 'PeakID', peak_names)\n",
    "    \n",
    "    return binary_df, category_counts\n",
    "\n",
    "\n",
    "import upsetplot  \n",
    "import ipdb\n",
    "\n",
    "\n",
    "def peak_motif_sets(\n",
    "    peak_file,\n",
    "    genome,\n",
    "    motif_list_file,\n",
    "    output_file=None,  # Optional parameter for output image base name\n",
    "    Motif_recount=False,\n",
    "    seperate_duplicates=False,\n",
    "    min_motif_set_count=0,\n",
    "    min_subset_count=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Process motif analysis and generate an UpSet plot and normalized bar graphs as image files.\n",
    "\n",
    "    Parameters:\n",
    "        peak_file (str): Path to the peak file.\n",
    "        genome (str): Genome reference.\n",
    "        motif_list_file (str): Path to the motif list file.\n",
    "        output_directory (str): Directory to save output files.\n",
    "        output_file (str, optional): Base path for saving the plots. The function appends\n",
    "                                     '_upset.png' and '_bar.png' to save distinct plots.\n",
    "                                     If None, plots are not saved. Defaults to None.\n",
    "        Motif_recount (bool, optional): Whether to force motif recount. Defaults to False.\n",
    "        seperate_duplicates (bool, optional): Whether to separate duplicates. Defaults to False.\n",
    "        min_motif_set_count (int, optional): Minimum number of motifs in a set. Defaults to 0.\n",
    "        min_subset_count (int, optional): Minimum subset size for plotting. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sorted data and the binary table.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    # os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Read motif headers\n",
    "    headers = read_motif_headers(motif_list_file)\n",
    "\n",
    "    # Get motif count\n",
    "    motif_count_table, motif_count_file = get_motif_count(\n",
    "        peak_file, genome, motif_list_file,  force=Motif_recount\n",
    "    )\n",
    "\n",
    "    # Format the motif count table\n",
    "    formatted_table = motif_count_table.iloc[:, -len(headers):].T\n",
    "    formatted_table.columns = motif_count_table.iloc[:, 0]\n",
    "    formatted_table.columns.name = 'PeakID'\n",
    "    formatted_table.index = headers\n",
    "\n",
    "    # Handle duplicate motifs if specified\n",
    "    if seperate_duplicates:\n",
    "        formatted_table = formatted_table.T\n",
    "        for item in subset_motifs:\n",
    "            condition = formatted_table[item] > 2\n",
    "            formatted_table.loc[condition, item] = 0\n",
    "            new_col = f'{item}_3+'\n",
    "            formatted_table[new_col] = 1 * condition\n",
    "        formatted_table = formatted_table.T\n",
    "        headers = formatted_table.index\n",
    "\n",
    "    # Create a binary table\n",
    "    binary_table = (formatted_table >= 0.5).astype(int)\n",
    "    df = binary_table.T\n",
    "    df_sorted = df.sort_values(by='PeakID')\n",
    "\n",
    "    # Rename columns if headers match\n",
    "    if len(headers) == len(df.columns):\n",
    "        df.columns = headers\n",
    "    else:\n",
    "        print(\"Error: The number of headers does not match the number of columns in the DataFrame.\")\n",
    "\n",
    "    # Generate UpSet data\n",
    "    df_upset = df.groupby(list(df.columns)).size().reset_index(name='count')\n",
    "    df_upset['num_motifs'] = df_upset.drop('count', axis=1).sum(axis=1)\n",
    "    filtered_df = df_upset[df_upset['num_motifs'] > min_motif_set_count]\n",
    "    filtered_df.set_index(df.columns.tolist(), inplace=True)\n",
    "    upset_data = filtered_df['count']\n",
    "\n",
    "    # Initialize saving mechanism based on output_file extension\n",
    "    save_as_images = False\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif']\n",
    "\n",
    "    if output_file:\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "        ext = ext.lower()\n",
    "        if ext in image_extensions:\n",
    "            save_as_images = True\n",
    "            # Prepare base name for image files\n",
    "            image_base = os.path.splitext(output_file)[0]\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        else:\n",
    "            print(f\"Unsupported output file extension: {ext}. Plots will not be saved.\")\n",
    "\n",
    "    # Generate UpSet plot\n",
    "    fig_upset = plt.figure(figsize=(20, 20), constrained_layout=True)\n",
    "    plot(\n",
    "        upset_data,\n",
    "        fig=fig_upset,\n",
    "        orientation=\"horizontal\",\n",
    "        show_counts=True,\n",
    "        min_subset_size=min_subset_count,\n",
    "        sort_categories_by=None\n",
    "    )\n",
    "\n",
    "    title_txt = (\n",
    "        f'Set intersections: [Membership > {min_motif_set_count}] '\n",
    "        f'[Counts > {min_subset_count}]\\n'\n",
    "        f'PEAKS: [{peak_file}]\\n'\n",
    "        f'MOTIFS: [{motif_list_file}]\\n'\n",
    "    )\n",
    "    fig_upset.suptitle(title_txt, fontsize=16)\n",
    "\n",
    "    # Save UpSet plot as image\n",
    "    if save_as_images:\n",
    "        upset_image_path = f\"{image_base}_upset.png\"\n",
    "        fig_upset.savefig(upset_image_path, bbox_inches=\"tight\")\n",
    "    #plt.close(fig_upset)  # Close the figure to free memory\n",
    "\n",
    "    # Prepare membership data\n",
    "    sets = {name: set(df[df[col] > 0].index) for name, col in zip(headers, df.columns)}\n",
    "    membership_df = pd.DataFrame(\n",
    "        {name: df.index.isin(indices) for name, indices in sets.items()}\n",
    "    )\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            itertools.combinations(headers, r) for r in range(1, len(headers) + 1)\n",
    "        )\n",
    "    )\n",
    "    ordered_combinations = sorted(all_combinations, key=lambda x: (len(x), x))\n",
    "\n",
    "    membership_matrix = {\n",
    "        ','.join(comb): df[list(comb)].all(axis=1) for comb in ordered_combinations\n",
    "    }\n",
    "    membership_df = pd.DataFrame(membership_matrix)\n",
    "    memberships = membership_df.apply(lambda row: list(membership_df.columns[row]), axis=1)\n",
    "    intersections = memberships.value_counts()\n",
    "\n",
    "    upset_data1 = all_possible_combinations33(df, min_set_count=min_motif_set_count)\n",
    "\n",
    "    # Process UpSet data\n",
    "    upset_data1.index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (len(x.split(',')), tuple(sorted(x.split(','))))\n",
    "            for x in upset_data1.index\n",
    "        ],\n",
    "        names=['Cardinality', 'Motifs']\n",
    "    )\n",
    "    upset_data1 = upset_data1[\n",
    "        upset_data1.index.get_level_values('Cardinality') >= min_motif_set_count\n",
    "    ]\n",
    "    upset_data1 = upset_data1.sort_index(ascending=False)\n",
    "    upset_data1.index = [', '.join(motifs) for _, motifs in upset_data1.index]\n",
    "\n",
    "    sorted_data = upset_data1.sort_values(ascending=True).tail(50)\n",
    "    total_sum = len(df)\n",
    "    binary_matrix = transform_combinations_to_matrix(sorted_data, peak_file)\n",
    "    sorted_binary_matrix = binary_matrix.sort_values(by='Count', ascending=False)\n",
    "    sorted_data = (sorted_data / total_sum * 10000).astype(int) / 100\n",
    "\n",
    "    # Generate normalized bar graphs\n",
    "    fig_bar = plt.figure(figsize=(20, 30), constrained_layout=True)\n",
    "    ax = sorted_data.plot(kind='barh', color='green', ax=plt.gca())\n",
    "    plt.title(\n",
    "        f'PEAKS [{total_sum}] : {peak_file}\\n'\n",
    "        f'MOTIFS: [{motif_list_file}]',\n",
    "        fontsize=16\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        'Top 50 co-occurring motif groups with # of motifs >= '\n",
    "        f'{min_motif_set_count}'\n",
    "    )\n",
    "    ax.set_xlabel('Co-occurrences (as % ratio of total peaks)')\n",
    "\n",
    "    for i, (value, index) in enumerate(zip(sorted_data, sorted_data.index)):\n",
    "        ax.text(\n",
    "            value,\n",
    "            i,\n",
    "            f' {value}, {round(total_sum * value / 100 + 0.5)}',\n",
    "            va='center',\n",
    "            ha='left',\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "    # Save bar graph as image\n",
    "    if save_as_images:\n",
    "        bar_image_path = f\"{image_base}_bar.png\"\n",
    "        fig_bar.savefig(bar_image_path, bbox_inches=\"tight\")\n",
    "    #plt.close(fig_bar)  # Close the figure to free memory\n",
    "\n",
    "    # Prepare return values\n",
    "    return_table = binary_table.T\n",
    "    return_data = sorted_data\n",
    "\n",
    "    return return_data, return_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from upsetplot import UpSet, from_memberships, plot\n",
    "\n",
    "# Configure environment variables\n",
    "os.environ[\"PATH\"] += (\n",
    "    \"/bioinformatics/homer/bin:\"\n",
    "    \"/usr/local/sbin:/usr/sbin:/usr/bin:/usr/local/bin:/usr/local/lib:\"\n",
    ")\n",
    "\n",
    "def read_motif_headers(motif_file):\n",
    "    \"\"\"Read headers from a motif file to extract motif names from header lines.\"\"\"\n",
    "    if is_denovo_motif_file(motif_file):\n",
    "        print('DENOVO HOMER')\n",
    "        return read_motif_headers_homer(motif_file)\n",
    "    else:\n",
    "        print('Not DENOVO HOMER')\n",
    "        return read_motif_headers_jaspar(motif_file)   \n",
    "\n",
    "def is_denovo_motif_file(filename):\n",
    "    \"\"\"\n",
    "    Reads a file and checks if any header lines starting with '>' contain \"BestGuess:\".\n",
    "\n",
    "    :param filename: str, the path to the file to be read\n",
    "    :return: bool, True if any header line contains \"BestGuess:\", otherwise False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('>') and \"BestGuess:\" in line:\n",
    "                    return True\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filename}' does not exist.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_motif_headers_jaspar(motif_file):\n",
    "    \"\"\"Read headers from a JASPAR motif file to extract motif names.\"\"\"\n",
    "    pattern = r'^>(\\w+)\\t'\n",
    "    motif_names = []\n",
    "    \n",
    "    with open(motif_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):  # Check if the line is a header\n",
    "                match = re.match(pattern, line)\n",
    "                if match:\n",
    "                    motif_names.append(match.group(1))  # Add the motif name to the list\n",
    "    \n",
    "    print(motif_names)\n",
    "    return motif_names\n",
    "\n",
    "def read_motif_headers_homer(motif_file):\n",
    "    \"\"\"Read headers from a HOMER motif file to extract motif names.\"\"\"\n",
    "    pattern = r\"BestGuess:([^/]+)\"\n",
    "    names = []\n",
    "    \n",
    "    with open(motif_file, 'r') as file:\n",
    "        headers = [line.strip() for line in file if line.startswith('>')]\n",
    "    \n",
    "    for item in headers:\n",
    "        names.extend(re.findall(pattern, item))\n",
    "    \n",
    "    return names \n",
    "\n",
    "def get_motif_count(peak_file, genome, motif_file, force=False):\n",
    "    \"\"\"Get motif count using HOMER annotatePeaks.\"\"\"\n",
    "    base_file = os.path.splitext(os.path.basename(peak_file))[0]\n",
    "    motif_base_file = os.path.splitext(os.path.basename(motif_file))[0]\n",
    "    \n",
    "    base_path = os.path.dirname(os.path.abspath(peak_file))\n",
    "    motif_count_file = os.path.join(base_path, f\"motifCount_counts_{base_file}_{motif_base_file}.txt\")\n",
    "    \n",
    "    print('Motif_Count_File=', motif_count_file, 'Force=', force)\n",
    "    command = (\n",
    "        f'annotatePeaks.pl {peak_file} {genome} -cpu 12 -noann -nogene '\n",
    "        f'-m {motif_file} -nmotifs > {motif_count_file}'\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(motif_count_file):\n",
    "        print('FILE DOES NOT EXIST:', motif_count_file)\n",
    "    \n",
    "    if not os.path.exists(motif_count_file) or force:\n",
    "        print(command)\n",
    "        print('Generating ' + motif_count_file)\n",
    "        os.system(command)\n",
    "    else:\n",
    "        print('** Loading ** ' + motif_count_file)\n",
    "    \n",
    "    return pd.read_table(motif_count_file, sep=\"\\t\"), motif_count_file\n",
    "\n",
    "def transform_combinations_to_matrix(series, peak_file):\n",
    "    \"\"\"Transform motif combinations into a binary matrix.\"\"\"\n",
    "    motifs = set()\n",
    "    for combo in series.index:\n",
    "        motifs.update(motif.strip() for motif in combo.split(','))\n",
    "    \n",
    "    motif_list = sorted(motifs)  # Sorting to maintain a consistent order\n",
    "    motif_list = list(set(motif_list))\n",
    "    \n",
    "    peak_data = pd.read_csv(peak_file, sep='\\t')\n",
    "    total_sum = len(peak_data)\n",
    "    \n",
    "    # Initialize a DataFrame to store the binary representation of combinations\n",
    "    data = {motif: [] for motif in motif_list}\n",
    "    data['Count'] = []\n",
    "    data['Norm_Count'] = []\n",
    "    data['Motif Subset'] = []\n",
    "    data[f'code-{peak_file}'] = []\n",
    "    \n",
    "    # Populate the DataFrame\n",
    "    for combo, count in series.items():\n",
    "        current_row = {motif: 0 for motif in motif_list}\n",
    "        current_row['Count'] = count\n",
    "        current_row['Norm_Count'] = count / total_sum\n",
    "        current_row['Motif Subset'] = ','.join(sorted(combo.split(',')))\n",
    "        binary_code = 0\n",
    "\n",
    "        for motif in motif_list:\n",
    "            binary_code <<= 1  # Shift left for each motif position\n",
    "            if motif in [motif.strip() for motif in combo.split(',')]:\n",
    "                current_row[motif] = 1\n",
    "                binary_code |= 1  # Set the last bit to 1 if the motif is present\n",
    "\n",
    "        current_row[f'code-{peak_file}'] = binary_code\n",
    "        for key, value in current_row.items():\n",
    "            data[key].append(value)\n",
    "\n",
    "    # Create DataFrame from data dictionary\n",
    "    result_df = pd.DataFrame(data)\n",
    "    return result_df\n",
    "\n",
    "def create_motif_network(data0, min_node_size=1000, max_node_size=5000):\n",
    "    \"\"\"\n",
    "    Create a network graph based on motif co-occurrences.\n",
    "\n",
    "    Args:\n",
    "        data0 (pd.DataFrame): DataFrame with columns representing motifs and a 'Count' column.\n",
    "        min_node_size (int, optional): Minimum size of the node bubbles. Defaults to 1000.\n",
    "        max_node_size (int, optional): Maximum size of the node bubbles. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = data0.copy()\n",
    "    data = data[data['Count'] > 0]\n",
    "    \n",
    "    # Assuming 'min_motif_set_count' is defined elsewhere or passed as a parameter\n",
    "    data = data[data.iloc[:, :-1].sum(axis=1) > min_motif_set_count]\n",
    "    \n",
    "    max_count = data['Count'].max()\n",
    "    data['normalized_count'] = data['Count'] / max_count\n",
    "    \n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes and edges based on co-occurrence\n",
    "    for _, row in data.iterrows():\n",
    "        motifs = [col for col in data.columns if row[col] == 1 and col not in ['Count', 'normalized_count']]\n",
    "        for i, motif1 in enumerate(motifs):\n",
    "            for motif2 in motifs[i+1:]:\n",
    "                if G.has_edge(motif1, motif2):\n",
    "                    G[motif1][motif2]['weight'] += row['Count']\n",
    "                else:\n",
    "                    G.add_edge(motif1, motif2, weight=row['Count']) \n",
    "    \n",
    "    # Calculate node sizes based on their total normalized co-occurrence count\n",
    "    node_weights = defaultdict(int)\n",
    "    for _, row in data.iterrows():\n",
    "        for motif in [col for col in data.columns if row[col] == 1 and col not in ['Count', 'normalized_count']]:\n",
    "            node_weights[motif] += row['normalized_count']\n",
    "    \n",
    "    # Scale node sizes between min_node_size and max_node_size\n",
    "    min_weight = min(node_weights.values())\n",
    "    max_weight = max(node_weights.values())\n",
    "    \n",
    "    node_size = [\n",
    "        min_node_size + (node_weights[motif] - min_weight) / (max_weight - min_weight) * (max_node_size - min_node_size)\n",
    "        for motif in G.nodes()\n",
    "    ]\n",
    "    \n",
    "    fixed_edge_width = 1\n",
    "    \n",
    "    # Calculate edge widths\n",
    "    if fixed_edge_width is not None:\n",
    "        edge_width = [fixed_edge_width for _ in G.edges()]\n",
    "    else:\n",
    "        edge_width = [G[u][v]['weight'] for u, v in G.edges()]                \n",
    "    \n",
    "    # Create labels with motif names and their total occurrences\n",
    "    labels = {motif: f\"{motif}: \\n {int(node_weights[motif] * max_count)}\" for motif in G.nodes()}\n",
    "    \n",
    "    # Draw the network\n",
    "    pos = nx.spring_layout(G, k=10/np.sqrt(G.order()), scale=2, iterations=2000, seed=1234)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue', edgecolors='red', linewidths=1.5)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    plt.title('Motif Co-occurrence Network')\n",
    "    plt.show()\n",
    "\n",
    "def all_possible_combinations33(df, min_set_count=0):\n",
    "    \"\"\"Generate all possible motif combinations with their counts.\"\"\"\n",
    "    motifs = df.columns.tolist()\n",
    "    combination_counts = {}\n",
    "    \n",
    "    # Count rows where none of the motifs are present\n",
    "    none_present_count = df.eq(0).all(axis=1).sum()\n",
    "    combination_counts['0_Set'] = none_present_count\n",
    "    \n",
    "    # Generate combinations \n",
    "    for r in range(1, len(motifs) + 1):\n",
    "        for comb in itertools.combinations(motifs, r):\n",
    "            comb_list = list(comb)\n",
    "            \n",
    "            if len(comb_list) >= min_set_count:\n",
    "                selected_motifs = df[comb_list].all(axis=1)\n",
    "                non_selected_motifs = df.drop(comb_list, axis=1, errors='ignore').eq(0).all(axis=1)\n",
    "                exact_match_mask = selected_motifs & non_selected_motifs\n",
    "                count = exact_match_mask.sum()\n",
    "                combination_key = ','.join(comb)\n",
    "                combination_counts[combination_key] = count\n",
    "    \n",
    "    return pd.Series(combination_counts)\n",
    "\n",
    "def calculate_all_motif_co_occurrences(df, max_distance):\n",
    "    \"\"\"Calculate all motif co-occurrences within a specified distance.\"\"\"\n",
    "    category_counts = defaultdict(int)\n",
    "    \n",
    "    # Collect the original order of motifs\n",
    "    original_motifs = df.columns[1:].tolist()  # Exclude the first column which is PeakID\n",
    "    motif_columns = {motif: idx for idx, motif in enumerate(original_motifs)}\n",
    "    \n",
    "    # Second pass to build the binary matrix\n",
    "    binary_matrix = []\n",
    "    peak_names = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        co_occurrences = find_co_occurring_motifs(row[1:], max_distance)  # Exclude PeakID column\n",
    "        \n",
    "        if co_occurrences:\n",
    "            base_peak_id = row[0].rsplit('-', 1)[0]  # Remove the last part after '-'\n",
    "            start_index = int(row[0].rsplit('-', 1)[1])  # Get the starting index (the number after '-')\n",
    "            \n",
    "            for k, occurrence in enumerate(co_occurrences):\n",
    "                binary_row = [0] * len(original_motifs)\n",
    "                for motif in occurrence:\n",
    "                    if motif in motif_columns:\n",
    "                        binary_row[motif_columns[motif]] = 1\n",
    "                category_counts[frozenset(occurrence)] += 1\n",
    "                \n",
    "                peak_id = f\"{base_peak_id}-{start_index + k}\"\n",
    "                binary_matrix.append(binary_row)\n",
    "                peak_names.append(peak_id)\n",
    "    \n",
    "    # Create DataFrame from the binary matrix\n",
    "    binary_df = pd.DataFrame(binary_matrix, columns=original_motifs)\n",
    "    binary_df.insert(0, 'PeakID', peak_names)\n",
    "    \n",
    "    return binary_df, category_counts\n",
    "\n",
    "def peak_motif_sets(\n",
    "    peak_file,\n",
    "    genome,\n",
    "    motif_list_file,\n",
    "    output_file=None,\n",
    "    Motif_recount=False,\n",
    "    seperate_duplicates=False,\n",
    "    min_motif_set_count=0,\n",
    "    min_subset_count=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Process motif analysis and generate an UpSet plot and normalized bar graphs as image files.\n",
    "\n",
    "    Parameters:\n",
    "        peak_file (str): Path to the peak file.\n",
    "        genome (str): Genome reference.\n",
    "        motif_list_file (str): Path to the motif list file.\n",
    "        output_file (str, optional): Base path for saving the plots. Defaults to None.\n",
    "        Motif_recount (bool, optional): Whether to force motif recount. Defaults to False.\n",
    "        seperate_duplicates (bool, optional): Whether to separate duplicates. Defaults to False.\n",
    "        min_motif_set_count (int, optional): Minimum number of motifs in a set. Defaults to 0.\n",
    "        min_subset_count (int, optional): Minimum subset size for plotting. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sorted data and the binary table.\n",
    "    \"\"\"\n",
    "    # Read motif headers\n",
    "    headers = read_motif_headers(motif_list_file)\n",
    "\n",
    "    # Get motif count\n",
    "    motif_count_table, motif_count_file = get_motif_count(\n",
    "        peak_file, genome, motif_list_file, force=Motif_recount\n",
    "    )\n",
    "\n",
    "    # Format the motif count table\n",
    "    formatted_table = motif_count_table.iloc[:, -len(headers):].T\n",
    "    formatted_table.columns = motif_count_table.iloc[:, 0]\n",
    "    formatted_table.columns.name = 'PeakID'\n",
    "    formatted_table.index = headers\n",
    "\n",
    "    # Handle duplicate motifs if specified\n",
    "    if seperate_duplicates:\n",
    "        formatted_table = formatted_table.T\n",
    "        for item in subset_motifs:\n",
    "            condition = formatted_table[item] > 2\n",
    "            formatted_table.loc[condition, item] = 0\n",
    "            new_col = f'{item}_3+'\n",
    "            formatted_table[new_col] = 1 * condition\n",
    "        formatted_table = formatted_table.T\n",
    "        headers = formatted_table.index\n",
    "\n",
    "    # Create a binary table\n",
    "    binary_table = (formatted_table >= 0.5).astype(int)\n",
    "    df = binary_table.T\n",
    "    df_sorted = df.sort_values(by='PeakID')\n",
    "\n",
    "    # Rename columns if headers match\n",
    "    if len(headers) == len(df.columns):\n",
    "        df.columns = headers\n",
    "    else:\n",
    "        print(\"Error: The number of headers does not match the number of columns in the DataFrame.\")\n",
    "\n",
    "    # Generate UpSet data\n",
    "    df_upset = df.groupby(list(df.columns)).size().reset_index(name='count')\n",
    "    df_upset['num_motifs'] = df_upset.drop('count', axis=1).sum(axis=1)\n",
    "    filtered_df = df_upset[df_upset['num_motifs'] > min_motif_set_count]\n",
    "    filtered_df.set_index(df.columns.tolist(), inplace=True)\n",
    "    upset_data = filtered_df['count']\n",
    "\n",
    "    # Initialize saving mechanism based on output_file extension\n",
    "    save_as_images = False\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif']\n",
    "\n",
    "    if output_file:\n",
    "        _, ext = os.path.splitext(output_file)\n",
    "        ext = ext.lower()\n",
    "        if ext in image_extensions:\n",
    "            save_as_images = True\n",
    "            # Prepare base name for image files\n",
    "            image_base = os.path.splitext(output_file)[0]\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        else:\n",
    "            print(f\"Unsupported output file extension: {ext}. Plots will not be saved.\")\n",
    "\n",
    "    # Generate UpSet plot\n",
    "    fig_upset = plt.figure(figsize=(20, 20), constrained_layout=True)\n",
    "    plot(\n",
    "        upset_data,\n",
    "        fig=fig_upset,\n",
    "        orientation=\"horizontal\",\n",
    "        show_counts=True,\n",
    "        min_subset_size=min_subset_count,\n",
    "        sort_categories_by=None\n",
    "    )\n",
    "\n",
    "    title_txt = (\n",
    "        f'Set intersections: [Membership > {min_motif_set_count}] '\n",
    "        f'[Counts > {min_subset_count}]\\n'\n",
    "        f'PEAKS: [{peak_file}]\\n'\n",
    "        f'MOTIFS: [{motif_list_file}]\\n'\n",
    "    )\n",
    "    fig_upset.suptitle(title_txt, fontsize=16)\n",
    "\n",
    "    # Save UpSet plot as image\n",
    "    if save_as_images:\n",
    "        upset_image_path = f\"{image_base}_upset.png\"\n",
    "        fig_upset.savefig(upset_image_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_upset)  # Close the figure to free memory\n",
    "\n",
    "    # Prepare membership data\n",
    "    sets = {name: set(df[df[col] > 0].index) for name, col in zip(headers, df.columns)}\n",
    "    membership_df = pd.DataFrame(\n",
    "        {name: df.index.isin(indices) for name, indices in sets.items()}\n",
    "    )\n",
    "\n",
    "    all_combinations = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            itertools.combinations(headers, r) for r in range(1, len(headers) + 1)\n",
    "        )\n",
    "    )\n",
    "    ordered_combinations = sorted(all_combinations, key=lambda x: (len(x), x))\n",
    "\n",
    "    membership_matrix = {\n",
    "        ','.join(comb): df[list(comb)].all(axis=1) for comb in ordered_combinations\n",
    "    }\n",
    "    membership_df = pd.DataFrame(membership_matrix)\n",
    "    memberships = membership_df.apply(lambda row: list(membership_df.columns[row]), axis=1)\n",
    "    intersections = memberships.value_counts()\n",
    "\n",
    "    upset_data1 = all_possible_combinations33(df, min_set_count=min_motif_set_count)\n",
    "\n",
    "    # Process UpSet data\n",
    "    upset_data1.index = pd.MultiIndex.from_tuples(\n",
    "        [\n",
    "            (len(x.split(',')), tuple(sorted(x.split(','))))\n",
    "            for x in upset_data1.index\n",
    "        ],\n",
    "        names=['Cardinality', 'Motifs']\n",
    "    )\n",
    "    upset_data1 = upset_data1[\n",
    "        upset_data1.index.get_level_values('Cardinality') >= min_motif_set_count\n",
    "    ]\n",
    "    upset_data1 = upset_data1.sort_index(ascending=False)\n",
    "    upset_data1.index = [', '.join(motifs) for _, motifs in upset_data1.index]\n",
    "\n",
    "    sorted_data = upset_data1.sort_values(ascending=True).tail(50)\n",
    "    total_sum = len(df)\n",
    "    binary_matrix = transform_combinations_to_matrix(sorted_data, peak_file)\n",
    "    sorted_binary_matrix = binary_matrix.sort_values(by='Count', ascending=False)\n",
    "    sorted_data = (sorted_data / total_sum * 10000).astype(int) / 100\n",
    "\n",
    "    # Generate normalized bar graphs\n",
    "    fig_bar = plt.figure(figsize=(20, 30), constrained_layout=True)\n",
    "    ax = sorted_data.plot(kind='barh', color='green', ax=plt.gca())\n",
    "    plt.title(\n",
    "        f'PEAKS [{total_sum}] : {peak_file}\\n'\n",
    "        f'MOTIFS: [{motif_list_file}]',\n",
    "        fontsize=16\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        'Top 50 co-occurring motif groups with # of motifs >= '\n",
    "        f'{min_motif_set_count}'\n",
    "    )\n",
    "    ax.set_xlabel('Co-occurrences (as % ratio of total peaks)')\n",
    "\n",
    "    for i, (value, index) in enumerate(zip(sorted_data, sorted_data.index)):\n",
    "        ax.text(\n",
    "            value,\n",
    "            i,\n",
    "            f' {value}, {round(total_sum * value / 100 + 0.5)}',\n",
    "            va='center',\n",
    "            ha='left',\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "    # Save bar graph as image\n",
    "    if save_as_images:\n",
    "        bar_image_path = f\"{image_base}_bar.png\"\n",
    "        fig_bar.savefig(bar_image_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig_bar)  # Close the figure to free memory\n",
    "\n",
    "    # Prepare return values\n",
    "    return_table = binary_table.T\n",
    "    return_data = sorted_data\n",
    "\n",
    "    return return_data, return_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENOVO HOMER\n",
      "Motif_Count_File= /home/psaisan/MCAT/motifCount_counts_KCN_H3K_FC2_1000_w200_KCH_VS_KCN_w400_L70.txt Force= False\n",
      "** Loading ** /home/psaisan/MCAT/motifCount_counts_KCN_H3K_FC2_1000_w200_KCH_VS_KCN_w400_L70.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8001/2721765511.py:379: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n",
      "  fig_upset.savefig(upset_image_path, bbox_inches=\"tight\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save results in : /home/psaisan/CARTMAN/Peak1_table.txt\n",
      "DENOVO HOMER\n",
      "Motif_Count_File= /home/psaisan/MCAT/motifCount_counts_KCH_H3K_FC2_1000_w200_KCH_VS_KCN_w400_L70.txt Force= False\n",
      "** Loading ** /home/psaisan/MCAT/motifCount_counts_KCH_H3K_FC2_1000_w200_KCH_VS_KCN_w400_L70.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8001/2721765511.py:379: UserWarning: There are no gridspecs with layoutgrids. Possibly did not call parent GridSpec with the \"figure\" keyword\n",
      "  fig_upset.savefig(upset_image_path, bbox_inches=\"tight\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save results in : /home/psaisan/CARTMAN/Peak2_table.txt\n"
     ]
    }
   ],
   "source": [
    "from upsetplot import plot\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# -------------------------\n",
    "# Configuration and Setup\n",
    "# -------------------------\n",
    "\n",
    "# Base directory path for data and outputs\n",
    "base_dir_in ='/home/psaisan/MCAT/'\n",
    "base_dir_out='/home/psaisan/CARTMAN/'\n",
    "\n",
    "\n",
    "\n",
    "# Genome reference\n",
    "GENOME = 'mm10'\n",
    "\n",
    "# Motif list file path\n",
    "motif_list_file = os.path.join(base_dir_in, 'KCH_VS_KCN_w400_L70.motifs')\n",
    "\n",
    "# Peak files\n",
    "peak_file1 = os.path.join(base_dir_in, 'KCN_H3K_FC2_1000_w200.txt')\n",
    "peak_file2 = os.path.join(base_dir_in, 'KCH_H3K_FC2_1000_w200.txt')\n",
    "\n",
    "# Output directories\n",
    "output_directory1 = os.path.join(base_dir_out, 'Example_Peak1/')\n",
    "output_directory2 = os.path.join(base_dir_out, 'Example_Peak2/')\n",
    "\n",
    "# CSV output paths for Peak 1 and 2\n",
    "csv_out_file1 = os.path.join(base_dir_out, 'Peak1_table.txt')\n",
    "csv_out_file2 = os.path.join(base_dir_out, 'Peak2_table.txt')\n",
    "\n",
    "# Image output base paths\n",
    "fig_output_path1 = os.path.join(base_dir_out, 'Peak1.png')  # Function appends _upset.png and _bar.png\n",
    "fig_output_path2 = os.path.join(base_dir_out, 'Peak2.png')  # Function appends _upset.png and _bar.png\n",
    "\n",
    "\n",
    "# Parameters with default values\n",
    "motif_recount = False\n",
    "seperate_duplicates = False\n",
    "min_motif_set_count = 0\n",
    "min_subset_count = 0\n",
    "\n",
    "# -------------------------\n",
    "# Processing Peak File 1\n",
    "# -------------------------\n",
    "\n",
    "# Call the peak_motif_sets function with appropriate parameters\n",
    "dp1, bdf1 = peak_motif_sets(\n",
    "    peak_file=peak_file1,\n",
    "    genome=GENOME,\n",
    "    motif_list_file=motif_list_file,\n",
    "    Motif_recount=motif_recount,\n",
    "    seperate_duplicates=seperate_duplicates,\n",
    "    min_motif_set_count=min_motif_set_count,\n",
    "    min_subset_count=min_subset_count,\n",
    "    output_file=fig_output_path1\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "# Save the binary table to a CSV file for Peak 1\n",
    "print('Save results in :',csv_out_file1)\n",
    "bdf1.to_csv(csv_out_file1, sep='\\t')\n",
    "\n",
    "# -------------------------\n",
    "# Processing Peak File 2\n",
    "# -------------------------\n",
    "\n",
    "# Call the peak_motif_sets function with appropriate parameters\n",
    "dp2, bdf2 = peak_motif_sets(\n",
    "    peak_file=peak_file2,\n",
    "    genome=GENOME,\n",
    "    motif_list_file=motif_list_file,\n",
    "    Motif_recount=motif_recount,\n",
    "    seperate_duplicates=seperate_duplicates,\n",
    "    min_motif_set_count=min_motif_set_count,\n",
    "    min_subset_count=min_subset_count,\n",
    "    output_file=fig_output_path2\n",
    ")\n",
    "\n",
    "\n",
    "# Save the binary table to a CSV file for Peak 2\n",
    "print('Save results in :',csv_out_file2)\n",
    "bdf2.to_csv(csv_out_file2, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect results: motif combinations table\n",
    "print('===')\n",
    "print('Motif co-occurance counts (normalized %) for '+peak_file1)\n",
    "print('===')\n",
    "print(dp1)\n",
    "\n",
    "print('===')\n",
    "print('Outuput File : ',csv_out_file1)\n",
    "print('===')\n",
    "display(pd.read_csv(csv_out_file1,delimiter='\\t'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glasslab",
   "language": "python",
   "name": "glasslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
